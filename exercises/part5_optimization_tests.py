import torch as t
from torch import nn, optim
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import make_moons
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np

def _get_moon_data(unsqueeze_y=False):
    X, y = make_moons(n_samples=512, noise=0.05, random_state=354)
    X = t.tensor(X, dtype=t.float32)
    y = t.tensor(y, dtype=t.int64)
    if unsqueeze_y:
        y = y.unsqueeze(-1)
    return DataLoader(TensorDataset(X, y), batch_size=128, shuffle=True)

def _train_with_opt(model, opt):
    dl = _get_moon_data()
    for i, (X, y) in enumerate(dl):
        opt.zero_grad()
        loss = F.cross_entropy(model(X), y)
        loss.backward()
        opt.step()

def _train_with_scheduler(model, opt, scheduler):
    dl = _get_moon_data()
    for epoch in range(20):
        for i, (X, y) in enumerate(dl):
            opt.zero_grad()
            loss = F.cross_entropy(model(X), y)
            loss.backward()
            opt.step()
        scheduler.step()

class Net(nn.Module):
    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(in_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, out_dim),
        )

    def forward(self, x: t.Tensor) -> t.Tensor:
        return self.layers(x)

class Net2(nn.Module):
    def __init__(self):
        super().__init__()
        self.base = nn.Sequential(nn.Linear(2, 5), nn.ReLU())
        self.classifier = nn.Sequential(nn.Linear(5, 3), nn.ReLU())
    def forward(self, x: t.Tensor) -> t.Tensor:
        return self.classifier(self.base(x))

def construct_param_config_from_description(description, model):
    param_config = []
    for param_group in description:
        param_group_ = param_group.copy()
        param_group_["params"] = getattr(model, param_group_["params"]).parameters()
        param_config.append(param_group_)
    return param_config

def test_sgd_param_groups(SGD):
    test_cases = [
        (
            [{'params': "base"}, {'params': "classifier", 'lr': 1e-3}],
            dict(lr=1e-2, momentum=0.0),
        ),
        (
            [{'params': "base"}, {'params': "classifier"}],
            dict(lr=1e-2, momentum=0.9),
        ),
        (
            [{'params': "base", "lr": 1e-2, "momentum": 0.95}, {'params': "classifier", 'lr': 1e-3}],
            dict(momentum=0.9, weight_decay=0.1),
        ),
    ]
    for description, kwargs in test_cases:
        t.manual_seed(819)

        model = Net2()
        param_config = construct_param_config_from_description(description, model)
        opt = optim.SGD(param_config, **kwargs)
        _train_with_opt(model, opt)
        w0_correct = model.base[0].weight
        
        t.manual_seed(819)
        model = Net2()
        param_config = construct_param_config_from_description(description, model)
        opt = SGD(param_config, **kwargs)
        _train_with_opt(model, opt)
        w0_submitted = model.base[0].weight

        print("\nTesting configuration: ", description)
        assert isinstance(w0_correct, t.Tensor)
        assert isinstance(w0_submitted, t.Tensor)
        t.testing.assert_close(w0_correct, w0_submitted, rtol=0, atol=1e-5)

    print("\nTesting that your function doesn't allow duplicates (this should raise an error): ")
    description, kwargs = (
        [{'params': "base", "lr": 1e-2, "momentum": 0.95}, {'params': "base", 'lr': 1e-3}],
        dict(momentum=0.9, weight_decay=0.1),
    )
    try:
        model = Net2()
        param_config = construct_param_config_from_description(description, model)
        opt = SGD(param_config, **kwargs)
    except:
        print("Got an error, as expected.\n")
    else:
        raise Exception("Should have gotten an error from using duplicate parameters, but didn't.")
    

    print("All tests in `test_sgd_param_groups` passed!")

def format_name(name):
    return name.replace("(", "<br>   ").replace(")", "").replace(", ", "<br>   ")

def format_config(config, line_breaks=False):
    if isinstance(config, dict):
        if line_breaks:
            s = "<br>   " + "<br>   ".join([f"{key}={value}" for key, value in config.items()])
        else:
            s = ", ".join([f"{key}={value}" for key, value in config.items()])
    else:
        param_config, args_config = config
        s = "[" + ", ".join(["{" + format_config(param_group_config) + "}" for param_group_config in param_config]) + "], " + format_config(args_config)
    return s

def test_sgd(SGD):

    test_cases = [
        dict(lr=0.1, momentum=0.0, weight_decay=0.0),
        dict(lr=0.1, momentum=0.7, weight_decay=0.0),
        dict(lr=0.1, momentum=0.5, weight_decay=0.0),
        dict(lr=0.1, momentum=0.5, weight_decay=0.05),
        dict(lr=0.2, momentum=0.8, weight_decay=0.05),
    ]
    for opt_config in test_cases:
        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = t.optim.SGD(model.parameters(), **opt_config)
        _train_with_opt(model, opt)
        w0_correct = model.layers[0].weight

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = SGD(model.parameters(), **opt_config)
        _train_with_opt(model, opt)
        w0_submitted = model.layers[0].weight

        print("\nTesting configuration: ", opt_config)
        assert isinstance(w0_correct, t.Tensor)
        assert isinstance(w0_submitted, t.Tensor)
        t.testing.assert_close(w0_correct, w0_submitted, rtol=0, atol=1e-5)


def test_rmsprop(RMSprop):

    test_cases = [
        dict(lr=0.1, alpha=0.9, eps=0.001, weight_decay=0.0, momentum=0.0),
        dict(lr=0.1, alpha=0.95, eps=0.0001, weight_decay=0.05, momentum=0.0),
        dict(lr=0.1, alpha=0.95, eps=0.0001, weight_decay=0.05, momentum=0.5),
        dict(lr=0.1, alpha=0.95, eps=0.0001, weight_decay=0.05, momentum=0.0),
    ]
    for opt_config in test_cases:
        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = t.optim.RMSprop(model.parameters(), **opt_config)
        _train_with_opt(model, opt)
        w0_correct = model.layers[0].weight

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = RMSprop(model.parameters(), **opt_config)
        _train_with_opt(model, opt)
        w0_submitted = model.layers[0].weight

        print("\nTesting configuration: ", opt_config)
        assert isinstance(w0_correct, t.Tensor)
        assert isinstance(w0_submitted, t.Tensor)
        t.testing.assert_close(w0_correct, w0_submitted, rtol=0, atol=1e-5)

def test_adam(Adam):

    test_cases = [
        dict(lr=0.1, betas=(0.8, 0.95), eps=0.001, weight_decay=0.0),
        dict(lr=0.1, betas=(0.8, 0.9), eps=0.001, weight_decay=0.05),
        dict(lr=0.2, betas=(0.9, 0.95), eps=0.01, weight_decay=0.08),
    ]
    for opt_config in test_cases:
        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = t.optim.Adam(model.parameters(), **opt_config)
        _train_with_opt(model, opt)
        w0_correct = model.layers[0].weight

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = Adam(model.parameters(), **opt_config)
        _train_with_opt(model, opt)
        w0_submitted = model.layers[0].weight

        print("\nTesting configuration: ", opt_config)
        assert isinstance(w0_correct, t.Tensor)
        assert isinstance(w0_submitted, t.Tensor)
        t.testing.assert_close(w0_correct, w0_submitted, rtol=0, atol=1e-5)

def get_sgd_optimizer(model, opt_config, SGD):
    if isinstance(opt_config, dict):
        return SGD(model.parameters(), **opt_config)
    else:
        opt_params = [d.copy() for d in opt_config[0]]
        _opt_config = opt_config[1]
        weight_params = [param for name, param in model.named_parameters() if "weight" in name]
        bias_params = [param for name, param in model.named_parameters() if "bias" in name]
        for param_group in opt_params:
            param_group["params"] = weight_params if param_group["params"] == "weights" else bias_params
        return SGD(opt_params, **_opt_config)

def test_ExponentialLR(ExponentialLR, SGD):

    print("Testing ExponentialLR, training loop has 30 epochs, 4 batches per epoch")


    test_cases = [
        dict(opt_config=dict(lr=0.01, momentum=0.0, weight_decay=0.0), scheduler_config=dict(gamma=1.0)),
        dict(opt_config=dict(lr=0.01, momentum=0.0, weight_decay=0.0), scheduler_config=dict(gamma=0.5)),
        dict(opt_config=dict(lr=0.01, momentum=0.9, weight_decay=0.1), scheduler_config=dict(gamma=0.5)),
    ]
    for config in test_cases:
        opt_config = config["opt_config"].copy()
        scheduler_config = config["scheduler_config"]

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = SGD(model.parameters(), **opt_config)
        scheduler = ExponentialLR(opt, **scheduler_config)
        _train_with_scheduler(model, opt, scheduler)
        w0_correct = model.layers[0].weight
        b0_correct = model.layers[0].bias

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = t.optim.SGD(model.parameters(), **opt_config)
        scheduler = optim.lr_scheduler.ExponentialLR(opt, **scheduler_config)
        _train_with_scheduler(model, opt, scheduler)
        w0_submitted = model.layers[0].weight
        b0_submitted = model.layers[0].bias

        print("\nTesting configuration:\n\toptimizer: ", format_config(opt_config), "\n\tscheduler: ", format_config(scheduler_config))
        assert isinstance(w0_correct, t.Tensor)
        assert isinstance(w0_submitted, t.Tensor)
        assert isinstance(b0_correct, t.Tensor)
        assert isinstance(b0_submitted, t.Tensor)
        t.testing.assert_close(w0_correct, w0_submitted, rtol=0, atol=1e-5)
        t.testing.assert_close(b0_correct, b0_submitted, rtol=0, atol=1e-5)
    print("\nAll tests in `test_ExponentialLR` passed!")

def test_StepLR(StepLR, SGD):

    print("Testing StepLR, training loop has 30 epochs, 4 batches per epoch")

    test_cases = [
        dict(opt_config=dict(lr=0.01, momentum=0.0, weight_decay=0.0), scheduler_config=dict(step_size=30, gamma=1.0)),
        dict(opt_config=dict(lr=0.01, momentum=0.0, weight_decay=0.0), scheduler_config=dict(step_size=3, gamma=1.0)),
        dict(opt_config=dict(lr=0.01, momentum=0.0, weight_decay=0.0), scheduler_config=dict(step_size=1, gamma=0.5)),
        dict(opt_config=dict(lr=0.01, momentum=0.9, weight_decay=0.1), scheduler_config=dict(step_size=3, gamma=0.5)),
    ]
    for config in test_cases:
        opt_config = config["opt_config"].copy()
        scheduler_config = config["scheduler_config"]

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = SGD(model.parameters(), **opt_config)
        scheduler = StepLR(opt, **scheduler_config)
        _train_with_scheduler(model, opt, scheduler)
        w0_correct = model.layers[0].weight
        b0_correct = model.layers[0].bias

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = t.optim.SGD(model.parameters(), **opt_config)
        scheduler = optim.lr_scheduler.StepLR(opt, **scheduler_config)
        _train_with_scheduler(model, opt, scheduler)
        w0_submitted = model.layers[0].weight
        b0_submitted = model.layers[0].bias

        print("\nTesting configuration:\n\toptimizer: ", format_config(opt_config), "\n\tscheduler: ", format_config(scheduler_config))
        assert isinstance(w0_correct, t.Tensor)
        assert isinstance(w0_submitted, t.Tensor)
        assert isinstance(b0_correct, t.Tensor)
        assert isinstance(b0_submitted, t.Tensor)
        t.testing.assert_close(w0_correct, w0_submitted, rtol=0, atol=1e-5)
        t.testing.assert_close(b0_correct, b0_submitted, rtol=0, atol=1e-5)
    print("\nAll tests in `test_StepLR` passed!")


def test_MultiStepLR(MultiStepLR, SGD):

    print("Testing MultiStepLR, training loop has 30 epochs, 4 batches per epoch")

    test_cases = [
        dict(opt_config=dict(lr=0.01, momentum=0.0, weight_decay=0.0), scheduler_config=dict(milestones=[40], gamma=1.0)),
        dict(opt_config=dict(lr=0.01, momentum=0.0, weight_decay=0.0), scheduler_config=dict(milestones=[10], gamma=0.5)),
        dict(opt_config=dict(lr=0.01, momentum=0.9, weight_decay=0.1), scheduler_config=dict(milestones=[10, 15], gamma=0.5)),
    ]
    for config in test_cases:
        opt_config = config["opt_config"].copy()
        scheduler_config = config["scheduler_config"]

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = SGD(model.parameters(), **opt_config)
        scheduler = MultiStepLR(opt, **scheduler_config)
        _train_with_scheduler(model, opt, scheduler)
        w0_correct = model.layers[0].weight
        b0_correct = model.layers[0].bias

        t.manual_seed(819)
        model = Net(2, 32, 2)
        opt = t.optim.SGD(model.parameters(), **opt_config)
        scheduler = optim.lr_scheduler.MultiStepLR(opt, **scheduler_config)
        _train_with_scheduler(model, opt, scheduler)
        w0_submitted = model.layers[0].weight
        b0_submitted = model.layers[0].bias

        print("\nTesting configuration:\n\toptimizer: ", format_config(opt_config), "\n\tscheduler: ", format_config(scheduler_config))
        assert isinstance(w0_correct, t.Tensor)
        assert isinstance(w0_submitted, t.Tensor)
        assert isinstance(b0_correct, t.Tensor)
        assert isinstance(b0_submitted, t.Tensor)
        t.testing.assert_close(w0_correct, w0_submitted, rtol=0, atol=1e-5)
        t.testing.assert_close(b0_correct, b0_submitted, rtol=0, atol=1e-5)
    print("\nAll tests in `test_MultiStepLR` passed!")

def plot_results(loss_list, accuracy_list):
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    fig.add_trace(go.Scatter(y=loss_list, name="Training loss"))
    fig.update_xaxes(title_text="Num batches observed")
    fig.update_yaxes(title_text="Training loss", secondary_y=False)
    # This next bit of code plots vertical lines corresponding to the epochs
    if len(accuracy_list) > 1:
        for idx, epoch_start in enumerate(np.linspace(0, len(loss_list), len(accuracy_list), endpoint=False)):
            fig.add_vline(x=epoch_start, line_width=3, line_dash="dash", annotation_text=f"Epoch {idx}", annotation_position="top right")
        fig.add_trace(
            go.Scatter(y=accuracy_list, x=np.linspace(0, len(loss_list), len(accuracy_list)), mode="lines", name="Accuracy"),
            secondary_y=True
        )
    fig.update_layout(template="simple_white", title_text="Training loss & accuracy on CIFAR10")
    fig.show()

def show_cifar_images(trainset, rows=3, cols=5):
    
    img = trainset.data[:rows*cols]
    fig = px.imshow(img, facet_col=0, facet_col_wrap=cols)
    for i, j in enumerate(np.arange(rows*cols).reshape(rows, cols)[::-1].flatten()):
            fig.layout.annotations[i].text = trainset.classes[trainset.targets[j]]
    fig.show()